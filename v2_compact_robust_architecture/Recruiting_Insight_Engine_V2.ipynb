{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PwC Recruiting Insight Engine – V2  \n",
    "## Training, Data Integrity & Diagnostics Notebook\n",
    "\n",
    "This notebook documents the end-to-end process for:\n",
    "\n",
    "1. Loading the raw HR datasets (`people.csv`, `salary.csv`, `descriptions.csv`)\n",
    "2. Merging them via the **Data Integrity Layer**\n",
    "3. Computing **merge KPIs** and the **Merge Health Index (MHI)**\n",
    "4. Gating model training based on MHI\n",
    "5. Training the salary prediction model (Random Forest + preprocessing pipeline)\n",
    "6. Saving artifacts (preprocessor + model) for the API + Insight Engine\n",
    "7. Running a small inference sanity check\n",
    "\n",
    "The notebook is aligned with the **V2 compact & robust architecture**, using the same modules as the production code:\n",
    "\n",
    "- `data_integrity.merge`\n",
    "- `data_integrity.kpis`\n",
    "- `data_integrity.mhi`\n",
    "- `data_integrity.diagnostics`\n",
    "- `model.train`\n",
    "- `model.inference`\n",
    "- `model.artifacts`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Optional: display options for debugging\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Ensure project root is on sys.path so we can import internal modules\n",
    "# -------------------------------------------------------------------\n",
    "PROJECT_ROOT = os.path.abspath(\".\")  # notebook at project root\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Import internal modules (V2 architecture)\n",
    "# -------------------------------------------------------------------\n",
    "from data_integrity.merge import merge_tables\n",
    "from data_integrity.kpis import compute_merge_kpis\n",
    "from data_integrity.mhi import compute_mhi\n",
    "from data_integrity.diagnostics import basic_merge_diagnostics\n",
    "\n",
    "from model.train import FEATURE_COLUMNS, TARGET_COLUMN\n",
    "from model.artifacts import save_artifacts\n",
    "from model.inference import load_pipeline, predict_one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load raw datasets\n",
    "\n",
    "We load the three raw CSV files provided in the challenge:\n",
    "\n",
    "- `people.csv` → demographic & job info\n",
    "- `salary.csv` → target variable\n",
    "- `descriptions.csv` → text descriptions (not used in V2 model, but used in integrity checks)\n",
    "\n",
    "We start with basic shape checks to ensure the files are readable and consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Paths relative to project root\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "PEOPLE_PATH = os.path.join(DATA_DIR, \"people.csv\")\n",
    "SALARY_PATH = os.path.join(DATA_DIR, \"salary.csv\")\n",
    "DESC_PATH = os.path.join(DATA_DIR, \"descriptions.csv\")\n",
    "\n",
    "people_df = pd.read_csv(PEOPLE_PATH)\n",
    "salary_df = pd.read_csv(SALARY_PATH)\n",
    "desc_df = pd.read_csv(DESC_PATH)\n",
    "\n",
    "print(\"people_df shape:\", people_df.shape)\n",
    "print(\"salary_df shape:\", salary_df.shape)\n",
    "print(\"desc_df shape:\", desc_df.shape)\n",
    "\n",
    "display(people_df.head())\n",
    "display(salary_df.head())\n",
    "display(desc_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge datasets via Data Integrity Layer\n",
    "\n",
    "We do **not** merge manually here.  \n",
    "Instead, we call `data_integrity.merge.merge_tables`, which encapsulates:\n",
    "\n",
    "- join logic\n",
    "- key alignment\n",
    "- type coercions (e.g., ID as integer)\n",
    "- defensive checks\n",
    "\n",
    "This keeps the notebook aligned with the production pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = merge_tables(people_df, salary_df, desc_df)\n",
    "\n",
    "print(\"merged_df shape:\", merged_df.shape)\n",
    "display(merged_df.head())\n",
    "\n",
    "print(\"\\nColumns:\", list(merged_df.columns))\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(merged_df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute merge KPIs & diagnostics\n",
    "\n",
    "We now compute:\n",
    "\n",
    "- **KPIs** via `compute_merge_kpis`:\n",
    "  - KAS, SCS, MDC, JSR, CCR_mean, MER_norm, DFC_norm, MDS, …\n",
    "\n",
    "- **Diagnostics** via `basic_merge_diagnostics`:\n",
    "  - row counts\n",
    "  - missing values\n",
    "  - duplicates\n",
    "  - sample preview\n",
    "\n",
    "These feed directly into the **Merge Health Index (MHI)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "kpis = compute_merge_kpis(people_df, salary_df, desc_df, merged_df)\n",
    "diagnostics = basic_merge_diagnostics(merged_df)\n",
    "\n",
    "print(\"=== KPIs ===\")\n",
    "for k, v in kpis.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== Diagnostics (summary) ===\")\n",
    "for k, v in diagnostics.items():\n",
    "    if k != \"sample_preview\":\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\nSample preview from diagnostics:\")\n",
    "pd.DataFrame(diagnostics[\"sample_preview\"]).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Merge Health Index (MHI)\n",
    "\n",
    "Using the KPIs, we compute the **MHI** via `data_integrity.mhi.compute_mhi`.\n",
    "\n",
    "Reminder of the structure:\n",
    "\n",
    "- `Gate`  → binary (0/1) based on key/schema/determinism\n",
    "- `Core`  → √(JSR × CCR_mean)\n",
    "- `Refinement` → exponential penalty based on MER_norm, DFC_norm, drift\n",
    "- `MHI`  → Gate × Core × Refinement\n",
    "- `zone` → RED / YELLOW / GREEN\n",
    "\n",
    "If **Gate = 0** or **zone = RED**, training should be aborted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mhi_result = compute_mhi(kpis)\n",
    "\n",
    "print(\"=== MHI RESULT ===\")\n",
    "for k, v in mhi_result.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "zone = mhi_result.get(\"zone\", \"UNKNOWN\")\n",
    "mhi_value = mhi_result.get(\"MHI\", None)\n",
    "\n",
    "print(\"\\nMHI zone:\", zone)\n",
    "if mhi_value is not None:\n",
    "    print(\"MHI score:\", round(mhi_value, 3))\n",
    "\n",
    "if zone == \"RED\" or mhi_result.get(\"Gate\", 0) == 0:\n",
    "    raise RuntimeError(\n",
    "        f\"❌ Training should NOT proceed: MHI={mhi_value:.3f}, zone={zone}, Gate={mhi_result.get('Gate')}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n✅ MHI is acceptable for training (YELLOW/GREEN).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare training dataset\n",
    "\n",
    "We now:\n",
    "\n",
    "1. Drop rows with missing target (`Salary`)\n",
    "2. Use the same `FEATURE_COLUMNS` and `TARGET_COLUMN` as the production pipeline\n",
    "3. Inspect basic statistics of the final training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Rows before dropping missing Salary:\", len(merged_df))\n",
    "\n",
    "clean_df = merged_df.dropna(subset=[TARGET_COLUMN])\n",
    "print(\"Rows after dropping missing Salary:\", len(clean_df))\n",
    "\n",
    "X = clean_df[FEATURE_COLUMNS].copy()\n",
    "y = clean_df[TARGET_COLUMN].copy()\n",
    "\n",
    "print(\"\\nFeature columns:\", FEATURE_COLUMNS)\n",
    "print(\"Target column:\", TARGET_COLUMN)\n",
    "\n",
    "display(X.head())\n",
    "print(\"\\nTarget summary:\")\n",
    "display(y.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train model via production training logic\n",
    "\n",
    "To stay consistent with the V2 architecture and avoid code drift,\n",
    "we **reuse the training logic** from `model.train`.\n",
    "\n",
    "Instead of re-implementing the pipeline here, we call `train_model()` and\n",
    "capture its summary (MHI, KPIs, diagnostics) while it:\n",
    "\n",
    "- builds the preprocessing pipeline\n",
    "- trains the RandomForestRegressor\n",
    "- saves artifacts (`preprocessor.pkl`, `model.pkl`) into `model/artifacts/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from model.train import train_model\n",
    "\n",
    "training_summary = train_model()\n",
    "\n",
    "print(\"\\n=== TRAINING SUMMARY (from train_model) ===\")\n",
    "for section, content in training_summary.items():\n",
    "    print(f\"\\n[{section.upper()}]\")\n",
    "    print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load artifacts and run a small inference sanity check\n",
    "\n",
    "Now we confirm that:\n",
    "\n",
    "1. Artifacts were saved correctly to `model/artifacts/`\n",
    "2. We can load the pipeline with `model.inference.load_pipeline`\n",
    "3. We can call `predict_one()` for a sample candidate profile\n",
    "\n",
    "This closes the loop: **Data Integrity → Training → Artifacts → Inference.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load trained pipeline\n",
    "pipeline = load_pipeline()\n",
    "\n",
    "# Take a real row from the cleaned dataset as a candidate example\n",
    "sample_row = clean_df.iloc[0]\n",
    "candidate_example = {\n",
    "    \"Age\": int(sample_row[\"Age\"]),\n",
    "    \"Gender\": str(sample_row[\"Gender\"]),\n",
    "    \"Education Level\": str(sample_row[\"Education Level\"]),\n",
    "    \"Job Title\": str(sample_row[\"Job Title\"]),\n",
    "    \"Years of Experience\": float(sample_row[\"Years of Experience\"]),\n",
    "}\n",
    "\n",
    "print(\"Candidate example:\", candidate_example)\n",
    "\n",
    "predicted_salary = float(predict_one(pipeline, candidate_example))\n",
    "print(\"\\nPredicted salary for this example:\", round(predicted_salary, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Notebook summary\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "1. Loaded the raw HR datasets (`people`, `salary`, `descriptions`)\n",
    "2. Merged them using the dedicated **Data Integrity Layer**\n",
    "3. Computed **KPIs** and the **MHI**, gating training based on data quality\n",
    "4. Prepared a clean training set with well-defined **features** and **target**\n",
    "5. Trained the model using the production `train_model()` flow\n",
    "6. Saved and reloaded artifacts for downstream use\n",
    "7. Ran a small inference sanity check to validate the end-to-end pipeline\n",
    "\n",
    "This notebook serves as:\n",
    "\n",
    "- A **transparent, reproducible training log**\n",
    "- A **technical companion** to the V2 architecture\n",
    "- A **debug tool** to inspect data integrity and model behavior\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
